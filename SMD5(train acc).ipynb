{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train acc로 evaluation method 전환.\n",
    "각 batch 마다, top k masks에 저장.\n",
    "exploit시에 top k masks에서 가져오기.\n",
    "\n",
    "batch별 train, test 구현.\n",
    "test acc로 evaluation method 전환."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter configurations\n",
    "exp_num = -1     # To save the result, change every time. -1 to not save.\n",
    "total_epoch = 10\n",
    "learning_rate = 0.001\n",
    "dropout_prob = 0.5\n",
    "top_k = 1\n",
    "if exp_num!=-1: os.mkdir(f'./Result{exp_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_MNIST():\n",
    "  transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
    "  train_dataset = MNIST(root='../data', train=True, download=True, transform=transform)\n",
    "  test_dataset = MNIST(root='../data', train=False, download=True, transform=transform)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "  return train_loader, test_loader\n",
    "\n",
    "def dataset_FMNIST():\n",
    "  transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.2860,), (0.3530,))])\n",
    "  train_dataset = FashionMNIST(root='../data', train=True, download=True, transform=transform)\n",
    "  test_dataset = FashionMNIST(root='../data', train=False, download=True, transform=transform)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "  return train_loader, test_loader\n",
    "\n",
    "def dataset_CIFAR10():\n",
    "  transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "  train_dataset = CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "  test_dataset = CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "  return train_loader, test_loader\n",
    "\n",
    "def dataset_CIFAR100():\n",
    "  transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
    "  train_dataset = CIFAR100(root='../data', train=True, download=True, transform=transform)\n",
    "  test_dataset = CIFAR100(root='../data', train=False, download=True, transform=transform)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "  test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "  return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run(dataset, method):\n",
    "    # Choose dataset\n",
    "    if dataset=='MNIST': train_loader, test_loader = dataset_MNIST()\n",
    "    elif dataset=='FMNIST': train_loader, test_loader = dataset_FMNIST()\n",
    "    elif dataset=='CIFAR10': train_loader, test_loader = dataset_CIFAR10()\n",
    "    elif dataset=='CIFAR100': train_loader, test_loader = dataset_CIFAR100()\n",
    "    print('Finished loading dataset')\n",
    "    \n",
    "    # Save log\n",
    "    if exp_num!=-1:\n",
    "        sys.stdout = open(f'./Result{exp_num}/{dataset}_{method}.txt', 'w')\n",
    "        sys.stdout = open(f'./Result{exp_num}/{dataset}_{method}.txt', 'a')\n",
    "    \n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    for epoch in range(total_epoch):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data, epoch)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_correct += (output.argmax(1) == target).sum().item()\n",
    "            model.record_acc((output.argmax(1) == target).sum().item())\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "        train_acc_list.append(train_accuracy)\n",
    "        train_loss_list.append(train_loss)\n",
    "        \n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (data, target) in enumerate(test_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data, epoch)\n",
    "                loss = criterion(output, target)\n",
    "                test_loss += loss.item()\n",
    "                test_correct += (output.argmax(1) == target).sum().item()\n",
    "            test_loss /= len(test_loader.dataset)\n",
    "            test_accuracy = test_correct / len(test_loader.dataset)\n",
    "            test_acc_list.append(test_accuracy)\n",
    "            test_loss_list.append(test_loss)\n",
    "            \n",
    "        # Show Result\n",
    "        print(f'Epoch {epoch + 1:2d} | '\n",
    "            f'Train Loss: {train_loss:.4f} | Train Accuracy: {round(train_accuracy*100, 2)} | '\n",
    "            f'Test Loss: {test_loss:.4f} | Test Accuracy: {round(test_accuracy*100, 2)}')\n",
    "    return train_loss_list, train_acc_list, test_loss_list, test_acc_list\n",
    "\n",
    "\n",
    "# 그래프\n",
    "def plot(dataset, method, exp_num):\n",
    "    figname = f\"[{dataset}] {method}  #{exp_num}.svg\"\n",
    "    plt.figure(figsize=(10, 6), dpi=100)\n",
    "    plt.plot(train_acc_list, label='Train_acc')\n",
    "    plt.plot(test_acc_list, label='Test_acc')\n",
    "    plt.plot(train_loss_list, label='Train_loss')\n",
    "    plt.plot(test_loss_list, label='Test_loss')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    if exp_num != -1 : plt.savefig(f'./Result{exp_num}/'+figname, format='svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMD(nn.Module):\n",
    "    def __init__(self, p=0.5, total_epoch=10, k=1):\n",
    "        super(SMD, self).__init__()\n",
    "        self.p = p\n",
    "        self.total_epoch = total_epoch\n",
    "        self.topk_masks = []\n",
    "        self.mask = None\n",
    "        self.accs = []\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x, curr_epoch):\n",
    "        if self.training:\n",
    "            if curr_epoch <= (self.total_epoch/2):\n",
    "                self.mask = (torch.rand_like(x) > self.p).float().to(device)\n",
    "                x = x * self.mask / (1 - self.p)\n",
    "                x = x.to(device)\n",
    "                return x\n",
    "            elif curr_epoch > (self.total_epoch/2):\n",
    "                self.mask = random.choice(self.topk_masks)\n",
    "                x = x * self.mask / (1 - self.p)\n",
    "                return x\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "    def record_acc(self, acc):\n",
    "        if len(self.topk_masks) < self.k:\n",
    "            self.topk_masks.append(self.mask)\n",
    "            self.accs.append(acc)\n",
    "        elif len(self.topk_masks) >= self.k:    \n",
    "            lowest = min(self.accs)\n",
    "            if acc > lowest:\n",
    "                lowest_idx = self.accs.index(lowest)\n",
    "                self.topk_masks.pop(lowest_idx)\n",
    "                self.accs.pop(lowest_idx)\n",
    "                self.topk_masks.append(self.mask)\n",
    "                self.accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading dataset\n",
      "Epoch  1 | Train Loss: 0.0579 | Train Accuracy: 15.96 | Test Loss: 0.0254 | Test Accuracy: 45.02\n",
      "Epoch  2 | Train Loss: 0.0213 | Train Accuracy: 49.62 | Test Loss: 0.0167 | Test Accuracy: 61.31\n",
      "Epoch  3 | Train Loss: 0.0170 | Train Accuracy: 60.16 | Test Loss: 0.0156 | Test Accuracy: 64.15\n",
      "Epoch  4 | Train Loss: 0.0154 | Train Accuracy: 63.87 | Test Loss: 0.0149 | Test Accuracy: 65.51\n",
      "Epoch  5 | Train Loss: 0.0145 | Train Accuracy: 66.07 | Test Loss: 0.0139 | Test Accuracy: 67.86\n",
      "Epoch  6 | Train Loss: 0.0138 | Train Accuracy: 67.4 | Test Loss: 0.0137 | Test Accuracy: 68.62\n",
      "Epoch  7 | Train Loss: 0.0133 | Train Accuracy: 68.71 | Test Loss: 0.0133 | Test Accuracy: 69.14\n",
      "Epoch  8 | Train Loss: 0.0129 | Train Accuracy: 69.71 | Test Loss: 0.0128 | Test Accuracy: 70.03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\VSC\\ED\\SMD4(solve OOM).ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/VSC/ED/SMD4%28solve%20OOM%29.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m method \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mSMD\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/VSC/ED/SMD4%28solve%20OOM%29.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# 모델 학습\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/VSC/ED/SMD4%28solve%20OOM%29.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m train_loss_list, train_acc_list, test_loss_list, test_acc_list \u001b[39m=\u001b[39m Run(\u001b[39m'\u001b[39;49m\u001b[39mFMNIST\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mSMD\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\user\\VSC\\ED\\SMD4(solve OOM).ipynb Cell 11\u001b[0m in \u001b[0;36mRun\u001b[1;34m(dataset, method)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/VSC/ED/SMD4%28solve%20OOM%29.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m output \u001b[39m=\u001b[39m model(data, epoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/VSC/ED/SMD4%28solve%20OOM%29.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user/VSC/ED/SMD4%28solve%20OOM%29.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/VSC/ED/SMD4%28solve%20OOM%29.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/VSC/ED/SMD4%28solve%20OOM%29.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Net_FMNIST_VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_FMNIST_VGG, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, 1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 3, 1)\n",
    "        self.fc1 = nn.Linear(4*4*128, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 100)\n",
    "        self.smd1 = SMD(p=dropout_prob, total_epoch=total_epoch, k=top_k)\n",
    "        self.smd2 = SMD(p=dropout_prob, total_epoch=total_epoch, k=top_k)\n",
    "\n",
    "    def forward(self, x, curr_epoch):\n",
    "        # 1\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        # 2\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        # 3\n",
    "        x = x.view(-1, 4*4*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.smd1(x, curr_epoch)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.smd2(x, curr_epoch)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def record_acc(self, acc):\n",
    "        self.smd.record_acc(acc)\n",
    "        \n",
    "# 모델 초기화\n",
    "model = Net_FMNIST_VGG().to(device)\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "dataset = 'FMNIST'\n",
    "method = 'SMD'\n",
    "\n",
    "# 모델 학습\n",
    "train_loss_list, train_acc_list, test_loss_list, test_acc_list = Run('FMNIST', 'SMD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(dataset, method, exp_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
